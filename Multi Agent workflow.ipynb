{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39fd1948-b5c3-48c4-b10e-2ae7e8c83334",
   "metadata": {},
   "source": [
    "# A Multi-Agent Workflow\n",
    "\n",
    "In this lesson, you'll build a data agent that can perform web research, answer questions, and generate charts.\n",
    "\n",
    "Let's load the environment variables that define the OpenAI API and Tavily API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install-deps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following yanked versions: 0.0.8, 0.1.18, 0.2.29, 0.2.30, 0.2.31, 0.3.0, 0.4.4, 0.6.9\n",
      "ERROR: Could not find a version that satisfies the requirement langgraph==0.1.0 (from versions: 0.0.9, 0.0.10, 0.0.11, 0.0.12, 0.0.13, 0.0.14, 0.0.15, 0.0.16, 0.0.17, 0.0.18, 0.0.19, 0.0.20, 0.0.21, 0.0.22, 0.0.23, 0.0.24, 0.0.25, 0.0.26, 0.0.27, 0.0.28, 0.0.29, 0.0.30, 0.0.31, 0.0.32, 0.0.33, 0.0.34, 0.0.35, 0.0.36, 0.0.37, 0.0.38, 0.0.39, 0.0.40, 0.0.41, 0.0.42, 0.0.43, 0.0.44, 0.0.45, 0.0.46, 0.0.47, 0.0.48, 0.0.49, 0.0.50, 0.0.51, 0.0.52, 0.0.53, 0.0.54, 0.0.55, 0.0.56, 0.0.57, 0.0.58, 0.0.59, 0.0.60, 0.0.61, 0.0.62, 0.0.63, 0.0.64, 0.0.65, 0.0.66, 0.0.67, 0.0.68, 0.0.69, 0.1.1, 0.1.2, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.1.10, 0.1.11, 0.1.12, 0.1.13, 0.1.14, 0.1.15, 0.1.16, 0.1.17, 0.1.19, 0.2.0, 0.2.1, 0.2.2, 0.2.3, 0.2.4, 0.2.5a0, 0.2.5, 0.2.6, 0.2.7a0, 0.2.7, 0.2.8, 0.2.9, 0.2.10, 0.2.11, 0.2.12, 0.2.13, 0.2.14, 0.2.15, 0.2.16, 0.2.17, 0.2.18, 0.2.19, 0.2.20, 0.2.21, 0.2.22, 0.2.23, 0.2.24, 0.2.25, 0.2.26, 0.2.27, 0.2.28, 0.2.32, 0.2.33, 0.2.34, 0.2.35, 0.2.36, 0.2.37, 0.2.38, 0.2.39, 0.2.40, 0.2.41, 0.2.42, 0.2.43, 0.2.44, 0.2.45, 0.2.46, 0.2.47, 0.2.48, 0.2.49, 0.2.50, 0.2.51, 0.2.52, 0.2.53, 0.2.54, 0.2.55, 0.2.56, 0.2.57, 0.2.58, 0.2.59, 0.2.60, 0.2.61, 0.2.62, 0.2.63, 0.2.64, 0.2.65, 0.2.66, 0.2.67, 0.2.68, 0.2.69, 0.2.70, 0.2.71, 0.2.72, 0.2.73, 0.2.74, 0.2.75, 0.2.76, 0.3.1, 0.3.2, 0.3.3, 0.3.4, 0.3.5, 0.3.6, 0.3.7, 0.3.8, 0.3.9, 0.3.10, 0.3.11, 0.3.12, 0.3.13, 0.3.14, 0.3.15, 0.3.16, 0.3.17, 0.3.18, 0.3.19, 0.3.20, 0.3.21, 0.3.22, 0.3.23, 0.3.24, 0.3.25, 0.3.26, 0.3.27, 0.3.28, 0.3.29, 0.3.30, 0.3.31, 0.3.32, 0.3.33, 0.3.34, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.5, 0.4.6, 0.4.7, 0.4.8, 0.4.9, 0.4.10, 0.5.0rc0, 0.5.0rc1, 0.5.0, 0.5.1, 0.5.2, 0.5.3, 0.5.4, 0.6.0a1, 0.6.0a2, 0.6.0, 0.6.1, 0.6.2, 0.6.3, 0.6.4, 0.6.5, 0.6.6, 0.6.7, 0.6.8, 0.6.10, 0.6.11, 1.0.0a1, 1.0.0a2, 1.0.0a3, 1.0.0a4, 1.0.0rc1, 1.0.0, 1.0.1, 1.0.2, 1.0.3)\n",
      "ERROR: No matching distribution found for langgraph==0.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install all required dependencies\n",
    "%pip install -q python-dotenv\n",
    "%pip install -q langchain==0.2.0\n",
    "%pip install -q langchain-openai==0.1.7\n",
    "%pip install -q langchain-community==0.2.0\n",
    "%pip install -q tavily-python==0.5.0\n",
    "%pip install -q langgraph==0.1.0\n",
    "%pip install -q matplotlib==3.9.2\n",
    "%pip install -q pandas==2.2.3\n",
    "%pip install -q seaborn==0.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2172e54",
   "metadata": {
    "height": 46
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "_ = load_dotenv(override=True)\n",
    "\n",
    "# Optional: Set API keys directly if not using .env file\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-openai-api-key'\n",
    "# os.environ['TAVILY_API_KEY'] = 'your-tavily-api-key'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dbe8c9",
   "metadata": {},
   "source": [
    "**Note**: These variables are already defined in this environment. If you'd like to run the notebook locally, you can define them in a `.env` file. For an env template, you can check the file `env.template` in this lesson's folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7551424b-6334-4c9b-8dc3-9c61a2347b62",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> üíª &nbsp; <b>To access <code>requirements.txt</code>, <code>env.template</code>, <code>prompts.py</code>, and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook 2) click on <em>\"Open\"</em>.\n",
    "\n",
    "<p> ‚¨á &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b6570f-c8ad-4176-8e25-94bc53a4a780",
   "metadata": {},
   "source": [
    "## 2.1 Initialize the agent's state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf984dd",
   "metadata": {},
   "source": [
    "State provides the agent with a shared, evolving memory across nodes so that the agents have the context and instructions needed to act coherently and achieve the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e65289f3",
   "metadata": {
    "height": 249
   },
   "outputs": [],
   "source": [
    "from typing import Literal, Optional, List, Dict, Any, Type\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# Custom State class with specific keys\n",
    "class State(MessagesState):\n",
    "    user_query: Optional[str] # The user's original query\n",
    "    enabled_agents: Optional[List[str]] # Makes our multi-agent system modular on which agents to include\n",
    "    plan: Optional[List[Dict[int, Dict[str, Any]]]] # Listing the steps in the plan needed to achieve the goal.\n",
    "    current_step: int # Marking the current step in the plan.\n",
    "    agent_query: Optional[str] # Inbox note: `agent_query` tells the next agent exactly what to do at the current step.\n",
    "    last_reason: Optional[str] # Explains the executor's decision to help maintain continuity and provide traceability.\n",
    "    replan_flag: Optional[bool] # Set by the executor to indicate that the planner should revise the plan.\n",
    "    replan_attempts: Optional[Dict[int, Dict[int, int]]] # Replan attempts tracked per step number.\n",
    "    final_answer: Optional[str] # The final synthesized answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba2b7fa-56e0-46dc-97c4-9e0a79ff0508",
   "metadata": {},
   "source": [
    "**Note**: `State` inherits from `MessagesState`, which is defined with a single `messages` key that keeps track of the list of messages shared among agents. So in addition to the fields you just defined for `State`, it also has a `messages` field from `MessagesState`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecbb6b7-62fe-4ac0-b557-36cfeb102897",
   "metadata": {},
   "source": [
    "## 2.2 Create planner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08cd82b",
   "metadata": {},
   "source": [
    "The planner takes in the user's query and generates a plan. The plan consists of a sequence of numbered steps; each step includes the action and the sub-agent that is assigned to that action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6116c152-2afa-43a3-92f0-6aeb4920988d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanMessage, SystemMessage\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Command\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m llm = ChatOpenAI(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m, temperature=\u001b[32m0\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplanner_node\u001b[39m(state: State) -> Command:\n\u001b[32m      9\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Creates or revises a plan based on the user query.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\Spark\\Lib\\site-packages\\langchain_core\\load\\serializable.py:113\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    112\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\envs\\Spark\\Lib\\site-packages\\pydantic\\v1\\main.py:347\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(__pydantic_self__, **data)\u001b[39m\n\u001b[32m    345\u001b[39m values, fields_set, validation_error = validate_model(__pydantic_self__.\u001b[34m__class__\u001b[39m, data)\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    349\u001b[39m     object_setattr(__pydantic_self__, \u001b[33m'\u001b[39m\u001b[33m__dict__\u001b[39m\u001b[33m'\u001b[39m, values)\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langgraph.types import Command\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def planner_node(state: State) -> Command:\n",
    "    \"\"\"Creates or revises a plan based on the user query.\"\"\"\n",
    "    \n",
    "    user_query = state.get(\"user_query\", \"\")\n",
    "    replan_flag = state.get(\"replan_flag\", False)\n",
    "    current_step = state.get(\"current_step\", 0)\n",
    "    enabled_agents = state.get(\"enabled_agents\", [])\n",
    "    \n",
    "    # Build the planning prompt\n",
    "    system_prompt = f\"\"\"\n",
    "    You are a planning agent. Create a step-by-step plan to answer the user's query.\n",
    "    Available agents: {enabled_agents}\n",
    "    \n",
    "    Return your plan as a JSON list where each step has:\n",
    "    - \"step\": step number (integer)\n",
    "    - \"agent\": which agent to use\n",
    "    - \"action\": what the agent should do\n",
    "    \n",
    "    Example format:\n",
    "    [\n",
    "        {{\"step\": 1, \"agent\": \"web_researcher\", \"action\": \"Search for information about...\"}},\n",
    "        {{\"step\": 2, \"agent\": \"chart_generator\", \"action\": \"Create a chart showing...\"}}\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    if replan_flag:\n",
    "        system_prompt += f\"\\n\\nThis is a replan. The previous plan failed at step {current_step}. Please revise the plan.\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=f\"User query: {user_query}\")\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    try:\n",
    "        # Extract JSON from the response\n",
    "        content = response.content\n",
    "        # Find JSON in the response\n",
    "        import re\n",
    "        json_match = re.search(r'\\[.*\\]', content, re.DOTALL)\n",
    "        if json_match:\n",
    "            plan = json.loads(json_match.group())\n",
    "        else:\n",
    "            plan = json.loads(content)\n",
    "    except:\n",
    "        # Fallback plan if parsing fails\n",
    "        plan = [\n",
    "            {\"step\": 1, \"agent\": \"web_researcher\", \"action\": f\"Search for: {user_query}\"},\n",
    "            {\"step\": 2, \"agent\": \"synthesizer\", \"action\": \"Summarize the findings\"}\n",
    "        ]\n",
    "    \n",
    "    print(f\"Planner created plan: {plan}\")\n",
    "    \n",
    "    return Command(\n",
    "        update={\n",
    "            \"plan\": plan,\n",
    "            \"current_step\": 1,\n",
    "            \"replan_flag\": False,\n",
    "            \"messages\": [HumanMessage(content=f\"Plan created: {plan}\", name=\"planner\")]\n",
    "        },\n",
    "        goto=\"executor\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executor-section",
   "metadata": {},
   "source": [
    "## 2.3 Create executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "executor-node",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "def executor_node(state: State) -> Command:\n",
    "    \"\"\"Executes the current step in the plan.\"\"\"\n",
    "    \n",
    "    plan = state.get(\"plan\", [])\n",
    "    current_step = state.get(\"current_step\", 1)\n",
    "    user_query = state.get(\"user_query\", \"\")\n",
    "    \n",
    "    # Find the current step in the plan\n",
    "    current_step_data = None\n",
    "    for step_dict in plan:\n",
    "        if step_dict.get(\"step\") == current_step:\n",
    "            current_step_data = step_dict\n",
    "            break\n",
    "    \n",
    "    if not current_step_data:\n",
    "        # No more steps, go to synthesizer\n",
    "        return Command(\n",
    "            update={\n",
    "                \"agent_query\": f\"Synthesize the answer for: {user_query}\",\n",
    "                \"messages\": [HumanMessage(content=\"All steps completed\", name=\"executor\")]\n",
    "            },\n",
    "            goto=\"synthesizer\"\n",
    "        )\n",
    "    \n",
    "    agent = current_step_data.get(\"agent\")\n",
    "    action = current_step_data.get(\"action\")\n",
    "    \n",
    "    print(f\"Executor: Step {current_step} - Agent: {agent}, Action: {action}\")\n",
    "    \n",
    "    return Command(\n",
    "        update={\n",
    "            \"agent_query\": action,\n",
    "            \"current_step\": current_step + 1,\n",
    "            \"messages\": [HumanMessage(content=f\"Executing step {current_step}: {action}\", name=\"executor\")]\n",
    "        },\n",
    "        goto=agent\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "web-researcher-section",
   "metadata": {},
   "source": [
    "## 2.4 Create web researcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "web-researcher-node",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "def web_research_node(state: State) -> Command:\n",
    "    \"\"\"Performs web research using Tavily.\"\"\"\n",
    "    \n",
    "    agent_query = state.get(\"agent_query\", \"\")\n",
    "    \n",
    "    print(f\"Web Researcher: Searching for: {agent_query}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize Tavily search\n",
    "        search_tool = TavilySearchResults(max_results=5)\n",
    "        results = search_tool.invoke(agent_query)\n",
    "        \n",
    "        # Format results\n",
    "        formatted_results = \"Search Results:\\n\"\n",
    "        for i, result in enumerate(results, 1):\n",
    "            formatted_results += f\"\\n{i}. {result.get('content', '')}\\n\"\n",
    "            if 'url' in result:\n",
    "                formatted_results += f\"   Source: {result['url']}\\n\"\n",
    "        \n",
    "        print(f\"Web Researcher found {len(results)} results\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        formatted_results = f\"Web search failed: {str(e)}. Using mock data.\"\n",
    "        # Mock data for testing without API key\n",
    "        formatted_results = \"\"\"Search Results (Mock Data):\n",
    "        1. JPMorgan Chase - Market Cap: $550 billion\n",
    "        2. Bank of America - Market Cap: $280 billion  \n",
    "        3. Wells Fargo - Market Cap: $200 billion\n",
    "        4. Goldman Sachs - Market Cap: $130 billion\n",
    "        5. Morgan Stanley - Market Cap: $150 billion\n",
    "        \"\"\"\n",
    "    \n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [HumanMessage(content=formatted_results, name=\"web_researcher\")]\n",
    "        },\n",
    "        goto=\"executor\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chart-generator-section",
   "metadata": {},
   "source": [
    "## 2.5 Create chart generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chart-generator-node",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "def chart_node(state: State) -> Command:\n",
    "    \"\"\"Generates charts based on data from previous steps.\"\"\"\n",
    "    \n",
    "    agent_query = state.get(\"agent_query\", \"\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    print(f\"Chart Generator: Creating chart for: {agent_query}\")\n",
    "    \n",
    "    # Extract data from previous messages\n",
    "    data_text = \"\"\n",
    "    for msg in messages[-5:]:  # Look at last 5 messages\n",
    "        if hasattr(msg, 'content'):\n",
    "            data_text += msg.content + \"\\n\"\n",
    "    \n",
    "    # Try to extract numerical data\n",
    "    banks = []\n",
    "    market_caps = []\n",
    "    \n",
    "    # Pattern to extract bank names and market caps\n",
    "    patterns = [\n",
    "        r'([\\w\\s]+)\\s*[-‚Äì]\\s*Market Cap:\\s*\\$([\\d.]+)\\s*(billion|trillion)',\n",
    "        r'([\\w\\s]+):\\s*\\$([\\d.]+)\\s*(billion|trillion)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, data_text, re.IGNORECASE)\n",
    "        if matches:\n",
    "            for match in matches:\n",
    "                bank_name = match[0].strip()\n",
    "                value = float(match[1])\n",
    "                unit = match[2].lower()\n",
    "                if unit == 'trillion':\n",
    "                    value *= 1000\n",
    "                banks.append(bank_name)\n",
    "                market_caps.append(value)\n",
    "            break\n",
    "    \n",
    "    # If no data extracted, use default data\n",
    "    if not banks:\n",
    "        banks = ['JPMorgan Chase', 'Bank of America', 'Wells Fargo', 'Goldman Sachs', 'Morgan Stanley']\n",
    "        market_caps = [550, 280, 200, 130, 150]\n",
    "    \n",
    "    # Create the chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = plt.cm.Blues(np.linspace(0.4, 0.8, len(banks)))\n",
    "    bars = plt.bar(banks, market_caps, color=colors)\n",
    "    \n",
    "    plt.title('Market Capitalization of Top US Banks', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Bank', fontsize=12)\n",
    "    plt.ylabel('Market Cap (Billion USD)', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, market_caps):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,\n",
    "                f'${value}B', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the chart\n",
    "    chart_path = '/tmp/market_cap_chart.png'\n",
    "    plt.savefig(chart_path, dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    chart_description = f\"Chart created showing market capitalization for {len(banks)} banks. \"\n",
    "    chart_description += f\"Top bank: {banks[market_caps.index(max(market_caps))]} with ${max(market_caps)}B market cap.\"\n",
    "    \n",
    "    print(f\"Chart Generator: Chart saved to {chart_path}\")\n",
    "    \n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [HumanMessage(content=chart_description, name=\"chart_generator\")]\n",
    "        },\n",
    "        goto=\"chart_summarizer\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chart-summarizer-section",
   "metadata": {},
   "source": [
    "## 2.6 Create chart summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "chart-summarizer-node",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart_summary_node(state: State) -> Command:\n",
    "    \"\"\"Summarizes the chart that was generated.\"\"\"\n",
    "    \n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    # Get the chart description from the previous step\n",
    "    chart_info = \"\"\n",
    "    for msg in reversed(messages):\n",
    "        if hasattr(msg, 'name') and msg.name == \"chart_generator\":\n",
    "            chart_info = msg.content\n",
    "            break\n",
    "    \n",
    "    if not chart_info:\n",
    "        chart_info = \"No chart information available\"\n",
    "    \n",
    "    # Create a summary using the LLM\n",
    "    summary_prompt = f\"\"\"\n",
    "    Summarize this chart information in 2-3 sentences:\n",
    "    {chart_info}\n",
    "    \n",
    "    Focus on the key insights and trends shown in the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=summary_prompt)])\n",
    "    summary = response.content\n",
    "    \n",
    "    print(f\"Chart Summarizer: {summary}\")\n",
    "    \n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [HumanMessage(content=summary, name=\"chart_summarizer\")]\n",
    "        },\n",
    "        goto=\"executor\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthesizer-section",
   "metadata": {},
   "source": [
    "## 2.7 Create synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "synthesizer-node",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesizer_node(state: State) -> Command:\n",
    "    \"\"\"Synthesizes all information into a final answer.\"\"\"\n",
    "    \n",
    "    user_query = state.get(\"user_query\", \"\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    # Collect all relevant messages\n",
    "    relevant_msgs = []\n",
    "    for msg in messages:\n",
    "        if hasattr(msg, 'name') and msg.name in ['web_researcher', 'chart_generator', 'chart_summarizer']:\n",
    "            relevant_msgs.append(f\"{msg.name}: {msg.content}\")\n",
    "    \n",
    "    synthesis_instructions = \"\"\"\n",
    "    Create a comprehensive answer to the user's question based on all the information gathered.\n",
    "    Include:\n",
    "    - Key findings from web research\n",
    "    - Data visualizations created\n",
    "    - Main insights and conclusions\n",
    "    \n",
    "    Keep the response concise but informative.\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_prompt = [\n",
    "        HumanMessage(content=(\n",
    "            f\"User question: {user_query}\\n\\n\"\n",
    "            f\"{synthesis_instructions}\\n\\n\"\n",
    "            f\"Context:\\n\\n\" + \"\\n\\n---\\n\\n\".join(relevant_msgs)\n",
    "        ))\n",
    "    ]\n",
    "    \n",
    "    llm_reply = llm.invoke(summary_prompt)\n",
    "    answer = llm_reply.content.strip()\n",
    "    \n",
    "    print(f\"\\n=== FINAL ANSWER ===\\n{answer}\\n===================\")\n",
    "    \n",
    "    return Command(\n",
    "        update={\n",
    "            \"final_answer\": answer,\n",
    "            \"messages\": [HumanMessage(content=answer, name=\"synthesizer\")],\n",
    "        },\n",
    "        goto=END\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d810f00e",
   "metadata": {},
   "source": [
    "## 2.8 Build the agent graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb15bf4",
   "metadata": {
    "height": 249
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "workflow.add_node(\"executor\", executor_node)\n",
    "workflow.add_node(\"web_researcher\", web_research_node)\n",
    "workflow.add_node(\"chart_generator\", chart_node)\n",
    "workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
    "workflow.add_node(\"synthesizer\", synthesizer_node)\n",
    "\n",
    "workflow.add_edge(START, \"planner\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e783bb21",
   "metadata": {
    "height": 63
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")\n",
    "    print(\"Graph nodes:\", list(workflow.nodes.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3b1fe",
   "metadata": {},
   "source": [
    "## 2.9 Use the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e173629",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ </b> The following two queries might take <b>2-5 minutes</b> to output the results.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b0d76-b8be-4a30-80e4-ee470680dbd0",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> \n",
    "<p>üö® &nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Your results may differ from those shown in the video. For example: \n",
    "\n",
    "In the first query, the agent might decide to call the synthesizer instead of calling the chart generator. In this case, you will only see text summarizing the web search results instead of a chart, which means the agent's answer is not completely relevant to the user's query. This is what you'll learn how to evaluate in the next lessons.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f640f",
   "metadata": {
    "height": 283
   },
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "import json\n",
    "\n",
    "query = \"Chart the current market capitalization of the top 5 banks in the US?\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "state = {\n",
    "    \"messages\": [HumanMessage(content=query)],\n",
    "    \"user_query\": query,\n",
    "    \"enabled_agents\": [\"web_researcher\", \"chart_generator\", \n",
    "                       \"chart_summarizer\", \"synthesizer\"],\n",
    "}\n",
    "\n",
    "try:\n",
    "    result = graph.invoke(state)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Final Answer:\")\n",
    "    print(result.get(\"final_answer\", \"No final answer generated\"))\n",
    "except Exception as e:\n",
    "    print(f\"Error running graph: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03180c6",
   "metadata": {
    "height": 232
   },
   "outputs": [],
   "source": [
    "query = \"Identify current regulatory changes for the financial services industry in the US.\"\n",
    "print(f\"Query: {query}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "state = {\n",
    "    \"messages\": [HumanMessage(content=query)],\n",
    "    \"user_query\": query,\n",
    "    \"enabled_agents\": [\"web_researcher\", \"chart_generator\", \n",
    "                       \"chart_summarizer\", \"synthesizer\"],\n",
    "}\n",
    "\n",
    "try:\n",
    "    result = graph.invoke(state)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Final Answer:\")\n",
    "    print(result.get(\"final_answer\", \"No final answer generated\"))\n",
    "except Exception as e:\n",
    "    print(f\"Error running graph: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
